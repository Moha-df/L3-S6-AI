{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d4463d-7b4c-425a-a1ff-18519e9e7bfe",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Objectifs du TP\n",
    "\n",
    "Nous souhaitons mettre en oeuvre, *from scratch*, un arbre de décision pour la classification des données du jeu `Iris`. Plus spécifiquement, il va s'agir:\n",
    "\n",
    "* D'observer les données, comprendre leur nature et les manipuler (éventuellement les transformer) pour les utiliser dans un arbre de décision (apprentissage et utilisation de l'arbre).\n",
    "* De s'assurer que le fonctionnement d'un arbre de décision est compris, afin de pouvoir le mettre en oeuvre dans un programme.\n",
    "* D'observer les résultats obtenus et de les mettre en perspective avec ce que l'on sait des données (à défaut de réaliser une évaluation plus poussée du modèle conçu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29295561-8301-4532-b9ca-f57d99ac83c8",
   "metadata": {},
   "source": [
    "# 1. Rappels conceptuels sur les arbres de décision\n",
    "\n",
    "Le jeu de données `Iris` est constitué de 150 instances, chaque instance représentant un spécimen d'Iris décrit par quatre attributs et une classe. Techniquement, il y a donc cinq colonnes dans le fichier `iris.csv`.\n",
    "\n",
    "Construire un arbre de décision sur des données consiste à placer les différentes attributes en tant que noeuds à l'intérieur de l'arbre. Les attributs seront placés selon leur *pouvoir discriminant*, c'est-à-dire la quantité d'information qu'ils apportent pour déterminer qu'une instance $\\mathbf{x}$ est de classe $c$.\n",
    "\n",
    "Dans ce TP, nous choisissons de construire un arbre de décision **binaire**. Chaque noeud placé dans l'arbre aura donc deux enfants, sauf s'il s'agit d'une feuille."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdaedd8-3e47-483f-800f-49f6399318a7",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Comment évalue-t-on le pouvoir discriminant d'un attribut ? Rappelez la formule permettant de réaliser le calcul.\n",
    "2. Que peut-on dire du pouvoir discriminant placé à la racine de l'arbre, par rapport à celui des autres attributs ?\n",
    "3. Quelles sont les informations contenues dans les noeuds de l'arbre ?\n",
    "    1. Donc un arbre de décision permet de déterminer la classe d'une instance en réalisant une séquence de _____ ?\n",
    "4. Les attributs du jeu de données que nous utilisons prennent des valeurs réelles. Comment devrons-nous gérer ces attributs ? (indiquez et détaillez plusieurs façons de procéder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5a01f2-286a-437b-a08e-91f0e6049ca3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Réponses\n",
    "\n",
    "1. Le pouvoir discriminant d'un attribut est évalué par le gain d'information :\n",
    "  Gain(S, A) = Entropie(S) - Σ(|Sv|/|S| × Entropie(Sv))\n",
    "  où Entropie(S) = -Σ(pi × log2(pi))\n",
    "\n",
    "2. L'attribut à la racine possède le plus grand gain d'information parmi tous les attributs disponibles.\n",
    "\n",
    "3. Les nœuds contiennent :\n",
    "  - L'attribut utilisé pour la décision\n",
    "  - La valeur de division (split value)\n",
    "  - Des pointeurs vers les nœuds enfants\n",
    "  \n",
    "    - Un arbre de décision détermine la classe d'une instance en réalisant une séquence de tests.\n",
    "\n",
    "4. Gestion des attributs à valeurs réelles :\n",
    "  - Seuil binaire : séparation optimale créant deux branches (< seuil et ≥ seuil)\n",
    "  - Discrétisation en intervalles\n",
    "  - Normalisation des valeurs\n",
    "  - Arbres à branches multiples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e43280-6856-49cd-a963-2285f2bab6a0",
   "metadata": {},
   "source": [
    "# 2. Prise en main du code fourni\n",
    "\n",
    "Vous avez à disposition une base de code dans le fichier `DecisionTree.py` (vous pourrez l'utiliser comme module dans le présent notebook; cf. partie \"Mise en oeuvre\"). Des fonctions y sont déjà mises en oeuvre:\n",
    "\n",
    "1. `entropy(df, target_name)`\n",
    "2. `attribute_gain(df, attribute, target)`\n",
    "3. `best_attribute(df, attributes, target)`\n",
    "\n",
    "La classe `DecisionTree` est partiellement mise en oeuvre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c5315-6645-4a18-bff4-f563b1333100",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "1. Reformulez, en pseudo-code, les trois fonctions listées ci-dessus\n",
    "2. À quoi les variables d'instances de la classe `DecisionTree` correspondent-elles ? Comment seront-elles définies lors de la construction de l'arbre ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4c38dd-aa1a-4fc6-8ed5-515ce318d7dc",
   "metadata": {},
   "source": [
    "# Reponses\n",
    "\n",
    "## Pseudo-code des fonctions\n",
    "\n",
    "### 1. `entropy(df, target_name)`\n",
    "**But** : Calculer l'entropie d'un dataset pour un attribut cible donné.\n",
    "\n",
    "```python\n",
    "fonction entropy(df, target_name):\n",
    "    initialiser une variable entropy à 0\n",
    "    pour chaque valeur unique v dans df[target_name]:\n",
    "        calculer la probabilité p de v dans df[target_name]\n",
    "        entropy = entropy - p * log2(p)\n",
    "    retourner entropy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `attribute_gain(df, attribute, target)`\n",
    "**But** : Calculer le gain d'information d'un attribut par rapport à l'entropie de la cible.\n",
    "\n",
    "```python\n",
    "fonction attribute_gain(df, attribute, target):\n",
    "    entropy_initiale = entropy(df, target)\n",
    "    initialiser weighted_entropy à 0\n",
    "\n",
    "    pour chaque valeur unique v dans df[attribute]:\n",
    "        filtrer df pour obtenir subset où attribute = v\n",
    "        calculer la probabilité p de subset par rapport à df\n",
    "        weighted_entropy = weighted_entropy + (p * entropy(subset, target))\n",
    "\n",
    "    retourner entropy_initiale - weighted_entropy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `best_attribute(df, attributes, target)`\n",
    "**But** : Trouver l’attribut avec le plus grand gain d’information.\n",
    "\n",
    "```python\n",
    "fonction best_attribute(df, attributes, target):\n",
    "    initialiser best_attr à None\n",
    "    initialiser max_gain à -∞\n",
    "\n",
    "    pour chaque attribut a dans attributes:\n",
    "        gain = attribute_gain(df, a, target)\n",
    "        si gain > max_gain:\n",
    "            max_gain = gain\n",
    "            best_attr = a\n",
    "\n",
    "    retourner best_attr\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Variables d'instance de la classe `DecisionTree`\n",
    "\n",
    "La classe `DecisionTree` représente un arbre de décision. Ses variables d'instance sont :\n",
    "\n",
    "- **`attribute`** : L'attribut choisi pour diviser les données à ce niveau de l'arbre.\n",
    "- **`value`** : La valeur de l'attribut pour ce nœud spécifique.\n",
    "- **`children`** : Un dictionnaire contenant les sous-arbres (les branches issues de ce nœud).\n",
    "- **`result`** : La classe majoritaire (utilisée si le nœud est une feuille).\n",
    "\n",
    "### Définition lors de la construction de l’arbre\n",
    "- L'algorithme sélectionne le meilleur attribut avec `best_attribute()`.\n",
    "- Il crée un nœud avec cet attribut.\n",
    "- Pour chaque valeur de l'attribut, il crée une branche et applique récursivement l'algorithme sur le sous-ensemble correspondant.\n",
    "- Si tous les exemples ont la même classe ou si plus aucun attribut ne peut être sélectionné, il crée une feuille avec `result` contenant la classe majoritaire.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe4805-b90e-405a-899b-ca8b574dab03",
   "metadata": {},
   "source": [
    "# 3. Mise en oeuvre\n",
    "\n",
    "La cellule ci-dessous contient le code qui devra être exécuté lorsque vous aurez mis en oeuvre les deux fonctions vides dans le module `DecisionTree.py`.\n",
    "\n",
    "## Travail à réaliser\n",
    "\n",
    "1. Mettez en oeuvre la fonction `fit()` de la classe `DecisionTree`.\n",
    "    1. Quels sont les cas de base de votre fonctions ?\n",
    "    2. Que retournez-vous lorsque le cas de base est atteint ?\n",
    "    3. Comment les branches de l'arbre sont-elles construites ? <br><br>\n",
    "*Reponses :*\n",
    "* 1. depth >= max_depth ou len(df[target].unique()) == 1 ou len(attributes) == 0 ou best_gain == 0\n",
    "* 2. on retourne un nœud feuille (isLeaf=True)\n",
    "* 3. L'arbre est construit récursivement en choisissant le meilleur attribut et sa valeur de division, en créant un nœud de décision, en divisant les données en deux partitions (< et ≥ à la valeur de division), puis en appelant fit() sur chaque partition pour former les sous-arbres gauche et droit.\n",
    "    \n",
    "\n",
    "2. Mettez en oeuvre la fonction `predict()` de la classe `DecisionTree`.\n",
    "    1. Quel est le cas de base de votre fonction ?\n",
    "    2. Que retournez-vous lorsque le cas de base est atteint ?\n",
    "    3. Comment parcourez-vous votre arbre ? <br><br>\n",
    "*Reponses :*\n",
    "* 1. Le cas de base de la fonction : Le cas de base est atteint lorsqu'on arrive à un nœud feuille (quand isLeaf=True).\n",
    "* 2. On retournes trois elements\n",
    "    - La prédiction du nœud (contenant le décompte des classes)\n",
    "    - La classe prédite (celle avec le plus grand nombre d'instances)\n",
    "    - La proportion d'instances appartenant à la classe prédite (en pourcentage)\n",
    "* 3. Le parcours se fait récursivement en comparant la valeur de l'attribut de l'instance avec la valeur de division du nœud :\n",
    "    - Si la valeur de l'attribut est inférieure à la valeur de division, on parcourt la branche gauche\n",
    "    - Sinon, on parcourt la branche droite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85df5e0b-2e88-4e0f-82ae-690c9feda887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Attribute: petal_length  Split value: 3.0]\n",
      "> True\n",
      "- Class Iris-setosa Count: 43\n",
      "-\n",
      "> False\n",
      "-[Attribute: petal_length  Split value: 4.5]\n",
      "-> True\n",
      "-- Class Iris-versicolor Count: 26\n",
      "--\n",
      "-> False\n",
      "--[Attribute: petal_width  Split value: 1.4]\n",
      "--> True\n",
      "--- Class Iris-versicolor Count: 1\n",
      "---\n",
      "--> False\n",
      "---[Attribute: sepal_length  Split value: 5.4]\n",
      "---> True\n",
      "---- Class Iris-virginica Count: 1\n",
      "----\n",
      "---> False\n",
      "----[Attribute: petal_length  Split value: 4.8]\n",
      "----> True\n",
      "----- Class Iris-versicolor Count: 9\n",
      "-----\n",
      "----> False\n",
      "-----[Attribute: sepal_length  Split value: 5.9]\n",
      "-----> True\n",
      "------ Class Iris-virginica Count: 5\n",
      "------\n",
      "-----> False\n",
      "------[Attribute: sepal_width  Split value: 2.5]\n",
      "------> True\n",
      "------- Class Iris-virginica Count: 1\n",
      "-------\n",
      "------> False\n",
      "-------[Attribute: petal_width  Split value: 1.5]\n",
      "-------> True\n",
      "-------- Class Iris-virginica Count: 1\n",
      "--------\n",
      "-------> False\n",
      "-------- Class Iris-virginica Count: 36\n",
      "-------- Class Iris-versicolor Count: 5\n",
      "--------\n",
      "** Instance to predict: 12    Iris-setosa\n",
      "Name: class, dtype: object\n",
      "\n",
      "Predicted class: Iris-setosa (100.0)\n",
      "\n",
      "** Instance to predict: 55    Iris-versicolor\n",
      "Name: class, dtype: object\n",
      "\n",
      "Predicted class: Iris-versicolor (100.0)\n",
      "\n",
      "** Instance to predict: 141    Iris-virginica\n",
      "Name: class, dtype: object\n",
      "\n",
      "Predicted class: Iris-virginica (87.8048780487805)\n",
      "\n",
      "** Instance to predict: 64    Iris-versicolor\n",
      "Name: class, dtype: object\n",
      "\n",
      "Predicted class: Iris-versicolor (100.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import DecisionTree as dt\n",
    "\n",
    "def test_DecisionTree(tree, test_data):\n",
    "    \"\"\"\n",
    "    Tests the decision tree tree on test data\n",
    "    Parameters :\n",
    "        tree: fit decision tree (DecisionTree)\n",
    "        test_data: set of instances from the dataset (dataframe)\n",
    "    \"\"\"\n",
    "    #print(f\"DEBUG: Entering test decision tree function\")\n",
    "    if tree == None: return None\n",
    "    \n",
    "    #print(f\"DEBUG: tree not none\")\n",
    "    tree.print_tree()\n",
    "\n",
    "    # Test the tree on a bunch of random instances\n",
    "    for i in range(4):\n",
    "        instance = test_data.sample()\n",
    "        print(f\"** Instance to predict: {instance[target]}\\n\")\n",
    "        _, predicted_class, proportion = tree.predict(instance)\n",
    "        print(f\"Predicted class: {predicted_class} ({proportion})\\n\")\n",
    "    return None\n",
    "\n",
    "#print(f\"DEBUG: Entering \")\n",
    "# Load the data\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# Set a portion of data aside for testing\n",
    "df_test  = df.sample(frac=0.15, random_state=42) # 22 instances\n",
    "df_train = df.drop(df_test.index) # 128 instances\n",
    "\n",
    "# Get attributes and target\n",
    "target = 'class'\n",
    "attributes = list(df_train.columns)[:-1] # attributes minus the target\n",
    "\n",
    "# Instanciate and train a decision tree on df\n",
    "#print(\"DEBUG: Avant création de l'arbre\")\n",
    "tree = dt.DecisionTree()\n",
    "#print(\"DEBUG: Arbre créé, avant fit\")\n",
    "tree = tree.fit(df_train, target, attributes)\n",
    "#print(\"DEBUG: Après fit, tree =\", tree)\n",
    "\n",
    "# Now, let's test that tree\n",
    "test_DecisionTree(tree, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66ed494-c192-448e-ad8d-d6070f216fcf",
   "metadata": {},
   "source": [
    "# Annexe - Résultats attendu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2b25b-183e-4ddd-a1ce-632a21375db7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Affichage de l'arbre\n",
    "```\n",
    "[Attribute: petal_length  Split value: 3.0]\n",
    "> True\n",
    "- Class Iris-setosa Count: 43\n",
    "-\n",
    "> False\n",
    "-[Attribute: petal_width  Split value: 1.4]\n",
    "-> True\n",
    "-- Class Iris-versicolor Count: 23\n",
    "--\n",
    "-> False\n",
    "--[Attribute: sepal_length  Split value: 5.2]\n",
    "--> True\n",
    "--- Class Iris-virginica Count: 1\n",
    "---\n",
    "--> False\n",
    "---[Attribute: sepal_width  Split value: 2.5]\n",
    "---> True\n",
    "---- Class Iris-virginica Count: 1\n",
    "----\n",
    "---> False\n",
    "---- Class Iris-virginica Count: 42\n",
    "---- Class Iris-versicolor Count: 18\n",
    "----\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d2584b-fde0-4c5d-9fb5-838ed3101d99",
   "metadata": {},
   "source": [
    "### Test sur 4 instances du jeu de test\n",
    "```\n",
    "** Instance to predict: 73    Iris-versicolor\n",
    "Name: class, dtype: object\n",
    "\n",
    "Node attribute: petal_length  Split value: 3.0\n",
    "Instance's value for petal_length: 4.7\n",
    "-Node attribute: petal_width  Split value: 1.4\n",
    "-Instance's value for petal_width: 1.2\n",
    "--> Node prediction:\n",
    " class\n",
    "Iris-versicolor    23\n",
    "Name: count, dtype: int64\n",
    "Predicted class: Iris-versicolor (100.0)\n",
    "\n",
    "** Instance to predict: 76    Iris-versicolor\n",
    "Name: class, dtype: object\n",
    "\n",
    "Node attribute: petal_length  Split value: 3.0\n",
    "Instance's value for petal_length: 4.8\n",
    "-Node attribute: petal_width  Split value: 1.4\n",
    "-Instance's value for petal_width: 1.4\n",
    "--Node attribute: sepal_length  Split value: 5.2\n",
    "--Instance's value for sepal_length: 6.8\n",
    "---Node attribute: sepal_width  Split value: 2.5\n",
    "---Instance's value for sepal_width: 2.8\n",
    "----> Node prediction:\n",
    " class\n",
    "Iris-virginica     42\n",
    "Iris-versicolor    18\n",
    "Name: count, dtype: int64\n",
    "Predicted class: Iris-virginica (70.0)\n",
    "\n",
    "** Instance to predict: 110    Iris-virginica\n",
    "Name: class, dtype: object\n",
    "\n",
    "Node attribute: petal_length  Split value: 3.0\n",
    "Instance's value for petal_length: 5.1\n",
    "-Node attribute: petal_width  Split value: 1.4\n",
    "-Instance's value for petal_width: 2.0\n",
    "--Node attribute: sepal_length  Split value: 5.2\n",
    "--Instance's value for sepal_length: 6.5\n",
    "---Node attribute: sepal_width  Split value: 2.5\n",
    "---Instance's value for sepal_width: 3.2\n",
    "----> Node prediction:\n",
    " class\n",
    "Iris-virginica     42\n",
    "Iris-versicolor    18\n",
    "Name: count, dtype: int64\n",
    "Predicted class: Iris-virginica (70.0)\n",
    "\n",
    "** Instance to predict: 73    Iris-versicolor\n",
    "Name: class, dtype: object\n",
    "\n",
    "Node attribute: petal_length  Split value: 3.0\n",
    "Instance's value for petal_length: 4.7\n",
    "-Node attribute: petal_width  Split value: 1.4\n",
    "-Instance's value for petal_width: 1.2\n",
    "--> Node prediction:\n",
    " class\n",
    "Iris-versicolor    23\n",
    "Name: count, dtype: int64\n",
    "Predicted class: Iris-versicolor (100.0)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
